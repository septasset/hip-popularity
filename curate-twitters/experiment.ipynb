{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, os, sys, glob\n",
    "import json, csv, re, datetime\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump into daily tweets data(pickle): vid -> tweetCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bz2_csv_rows(fp):\n",
    "    with bz2.open(fp, mode='rt') as bzfp:\n",
    "        for line in tqdm(bzfp, desc=\"hugefile\"):\n",
    "            sp = line.split(',')\n",
    "            yield sp\n",
    "            \n",
    "# time: 3945000it [02:47, 23483.87it/s]\n",
    "# Size: 17.4MB\n",
    "def read_write_file(infile, outfile):\n",
    "    \"\"\"\n",
    "    {\n",
    "        vid_1: tweetCounts,\n",
    "        vid_2: tweetCounts,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    map_vid_tweetCounts = {}\n",
    "    all_vids = []    \n",
    "    all_dates = []\n",
    "\n",
    "    counts = 0    \n",
    "    for row in bz2_csv_rows(infile):\n",
    "        counts += 1\n",
    "        # TODO: test only\n",
    "        if int(counts / 100000) == 1:\n",
    "            break\n",
    "            \n",
    "        # ignore rate message rows\n",
    "        if len(row) < 10:\n",
    "            continue\n",
    "        \n",
    "        # get the date (yyyy-mm-dd)\n",
    "        date = row[1].strip()\n",
    "\n",
    "        vids = []\n",
    "        # single vid is of length 11\n",
    "        original_vids = row[7].strip()\n",
    "        if original_vids != 'N':\n",
    "            vids.extend(original_vids.split(\";\"))\n",
    "        retweeted_vids = row[8].strip()\n",
    "        if retweeted_vids != 'N':\n",
    "            vids.extend(retweeted_vids.split(\";\"))\n",
    "        quoted_vids = row[9].strip()\n",
    "        if quoted_vids != 'N':\n",
    "            vids.extend(quoted_vids.split(\";\"))\n",
    "        \n",
    "        for vid in vids:\n",
    "            if vid not in map_vid_tweetCounts:\n",
    "                map_vid_tweetCounts[vid] = 0\n",
    "            map_vid_tweetCounts[vid] += 1\n",
    "        \n",
    "        all_vids.append(vids)\n",
    "        all_dates.append(date)\n",
    "    \n",
    "    if outfile is not None:\n",
    "        pickle.dump(map_vid_tweetCounts, open(outfile, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirtodir_read_write(indir, outdir, date_range):\n",
    "    ap = {\n",
    "        \"indir\": indir,\n",
    "        \"outdir\": outdir,        \n",
    "    }\n",
    "\n",
    "    date_start = datetime.datetime.strptime(date_range[0], \"%Y-%m-%d\")\n",
    "    date_end   = datetime.datetime.strptime(date_range[1], \"%Y-%m-%d\")\n",
    "    \n",
    "    res = []\n",
    "    for infile in glob.glob(indir + \"/*.bz2\"):\n",
    "        date_str = os.path.split(infile)[1].split(\".\")[0]\n",
    "        date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        if date >= date_start and date <= date_end:\n",
    "            outfile = os.path.join(outdir, date_str + \".pik\")\n",
    "            res.append((infile, outfile))\n",
    "\n",
    "    Parallel(n_jobs=5)(delayed(read_write_file)(x[0], x[1]) for x in res[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_proc_dir = \"/data4/u5941758/yt_tweets_2015_2019/tweet_stats\"\n",
    "output_dir      = \"./output\"\n",
    "\n",
    "example_in_path  = os.path.join(tweets_proc_dir, \"2016-07-01.bz2\")\n",
    "example_out_path = os.path.join(output_dir, \"2016-06-30.pik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_write_file(example_in_path, example_out_path)\n",
    "dirtodir_read_write(tweets_proc_dir, output_dir, (\"2016-07-02\", \"2016-09-01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dumped daily data and integrate to get final output\n",
    "#### Final output\n",
    "```\n",
    "# category.pik/json\n",
    "{\n",
    "    vid_1: {\n",
    "        day_zero: 2016-06-30,\n",
    "        days: [],\n",
    "        tweets: []\n",
    "    },\n",
    "    vid_2: {\n",
    "        day_zero: 2016-06-30,\n",
    "        days: [],\n",
    "        tweets: []\n",
    "    }...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vids(inpath):\n",
    "    res = dict()\n",
    "    with open(inpath, 'r', encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row)==0: continue\n",
    "            category = row[0]\n",
    "            vids = row[1:]\n",
    "            res[category] = set(vids)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "engage_vids_path = \"../data/engage16/filtered/vids_filter(all).csv\"\n",
    "map_category_vids = read_vids(engage_vids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_category(vid, map_category_vids):\n",
    "    for cat, vid_set in map_category_vids.items():\n",
    "        if vid in vid_set:\n",
    "            return cat\n",
    "    return None\n",
    "\n",
    "def build_final(indir, vids_path):\n",
    "    dataset = dict()\n",
    "    \n",
    "    # engage vids\n",
    "    map_category_vids = read_vids(vids_path)\n",
    "    \n",
    "    for infile in glob.glob(indir + \"/*.bz2\"):\n",
    "        date_str = os.path.split(infile)[1].split(\".\")[0]\n",
    "        date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        date_aligned = date - datetime.timedelta(days=1)\n",
    "        \n",
    "        daily_tweets = pickle.load(open(infile, \"rb\"))\n",
    "        for vid, tweetCounts in daily_tweets.items():\n",
    "            cat = find_category(vid, map_category_vids) \n",
    "            if cat is None: continue\n",
    "            if cat not in dataset:\n",
    "                dataset[cat] = dict()\n",
    "            else:\n",
    "                # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hip-long",
   "language": "python",
   "name": "hip-long"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
